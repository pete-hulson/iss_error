---
title: "Inclusion of ageing error and growth variability in the estimation of age composition input sample size"
author:
  - name: Pete Hulson
    institute: afscjnu
    email: pete.hulson@noaa.gov
    correspondence: true
  - name: Benjamin Williams
    institute: afscjnu    
institute:
  - afscjnu: Auke Bay Laboratories, Alaska Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, 17109 Point Lena Loop Rd., Juneau, AK 99801
output:
 
  bookdown::word_document2:
    toc: false
    number_sections: false
    reference_docx: styles_reference.docx
    pandoc_args:  
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
bibliography: refs.bib   
csl: canjfas.csl
header-includes:
  - \usepackage(amsmath) # for cases in equations
  - \usepackage{booktabs}
  - \usepackage{cleveref}
  - \renewcommand{\eqref}{\Cref}
  - \Crefformat{equation}{#2#1#3}
  
editor_options: 
  markdown: 
    wrap: sentence
---

# Abstract

[Pete]

\newpage

# Introduction

Compositional information on age and length comprise critical data products used in statistical catch-at-age assessment models, as they facilitate the tracking of year classes and size-structure over time and improve our understanding of the population dynamics [@QuinnDeriso1999]. 
There are two primary sources for age and length composition data used in statistical catch-at-age models: fishery-independent sources, which include some level of randomized and standardized collection of samples in a non-targeted framework, and fishery-dependent sources, in which collection of age and length samples are also randomized at some level but are obtained from hauls that are not random but rather targeted within a specific fishery. 
Regardless of the source of age and length composition data, it is commonly accepted that overdispersion of collections are inherent to the data due to intra-haul correlation [e.g., @Pennington2000]. 
The concept of 'effective sample size' has since been developed, that is smaller than the sample size collected, to reflect the increased uncertainty due to this overdispersion [e.g., @Mcallister1997]. 
Since modern statistical catch-at-age stock assessment models integrate multiple sources of data related to catch (e.g., fisheries catch-per-unit-effort, survey indices of abundance), life history (e.g., size-at-age, maturity-at-age, selectivity-at-age), and composition (e.g., length and age), it is imperative to consider the relative information content these data products provide to the model employed. 

Because fisheries often depend on the periodic production of strong year-classes and subsequent recruitment into fishery catch, sampling efforts for age and length data, scaling of these data to the population level ('compositional expansion'), and the weight assigned to these data products in assessment models are highly important in order to provide accurate advice for management. 
This is often handled through the use of data-weighting methods, checking the fit of compositional data in the model, and ensuring a good match between the variance of the data and the variance implied by the model [@Francis2017]. 
The weight assigned to annual compositions (the 'input sample size') can follow a myriad of methods (e.g., fixed values as in @Monnahan2021, number of samples or tows sampled upon as in @Hulson2021 or @Spencer2022, bootstrapping compositions as in @Stewart2014). 
The primary consideration when assigning a weight to age or length composition data is to account for the potential variability and correlation in the sampling process. 
The method developed by @Stewart2014 allows for resampling techniques to be employed at each level of the sampling design, and provides an objective avenue to determine input sample size that is smaller than the actual sample size based on the observation variability contained within the sampling process.  

In addition to intra-haul correlation, for every fish species sampled for which age is capable of being determined from otoliths there is resulting variability in the ageing of the otolith, often called 'ageing error' [e.g., @Punt2008]. 
There are a number of factors that can influence the magnitude of ageing error [@Nesslage2022], but inherent to obtaining ages from otoliths is variability in the age readings. 
To account for this source of variability, ageing laboratories regularly evaluate precision through obtaining multiple readings of the same otolith across different age readers [@Morrison2005]. 
Several methods have been developed to account for ageing error in statistical catch-at-age models when fitting age composition through the use of an ageing error matrix [@Candy2012; @Punt2008]. 
The essence of this approach is that the numbers-at-age estimated by the assessment model are 'corrected' through multiplication with an ageing error matrix, which assigns estimated numbers-at-age to adjacent age-classes depending on the magnitude of the ageing error within the specific age-class. 
Since the development and implementation of ageing error matrices a number of studies have been devoted to quantifying the effects of ageing error on assessment model estimates [e.g., @Henriquez2016, @Liao2013]. 
Within each of these studies, and in each application of an ageing error matrix within a stock assessment model, the age composition data fit will be weighted by an input sample size. 
As described previously, the input sample size selected would reflect the variability in the sampling process, which would also include the variability in the age readings themselves. 

In the process of compositional expansion, it is often the case that an age-length key (ALK) is employed to expand population numbers-at-length to population numbers-at-age [@Ailloud2019]. 
It is through the age-length key, and the subsequent age expansion, that observations of age composition are derived from fishery-independent and fishery-dependent sources. 
Conditional age-at-length (CAAL), in which paired age-length data are used as in indication of the age distribution for a specific length, is used to inform length-at-age and it's related uncertainty [@Taylor2013]. 
CAAL data can be used directly within statistical catch-at-age models to inform estimates of growth as well as composition data [@Lee2019] and has been implemented in a number of operational stock assessments [e.g., @Mcgilliard2019; @Hulson2022b]. 
An intrinsic component to both the ALK and CAAL is the variability in length for a given age.
Further, when using CAAL data as an additional likelihood component to a statistical catch-at-age model one must determine the input sample size to be used to weight this information.

Despite the acceptance of requiring an input sample size to weight age composition data in statistical catch-at-age models that reflects the added uncertainty caused by overdispersion common to age sampling, and the recognition of the inherent variability in the ageing process when reading otoliths and in the growth process upon which age-length keys are based, these sources of uncertainty have not been previously integrated in an objective estimation method for input sample size. 
In this study, we use the methods of @Stewart2014 to estimate age composition input sample size that includes both ageing error and growth variability in the estimation process. 
We show, in a step-wise process, the added variability in age composition sample size from including ageing error and growth variability across a number of species that reflect differing life histories and levels of ageing difficulty. 
Using a current statistical catch-at-age models that are used for assessment and management as case studies we then show the effects of this added uncertainty on assessment model results.

# Methods

## Bottom trawl survey data

[Paras on survey background, sampling for length and ages, and sample sizes - Jason and Meaghan]

## Computing length and age composition

[Para on expansion methods, maybe refer to tech memos or include appendix - Pete and Matt]

## Simulation-Bootstrap framework

[Included what we had from tech memo to build on - Ben and Pete]

To evaluate the effect of sub-sampling length frequency collections for which sex is subsequently determined we developed a bootstrap-simulation framework that allowed for reductions in the number of sexed length frequencies collected.
We used the historical length frequency data that were collected from the bottom trawl surveys to evaluate the impact of reduced sex-specific length frequency data.
In simple terms, the simulation framework that we developed would select a pre-determined number of fish from the length frequency collections that would then be subsequently sexed, the remaining length frequency data (regardless of whether sex was actually determined in the historical data) was classified as 'unsexed'.

The bootstrap-simulation framework is composed of a suite of nested resampling protocols.
Bootstrap resampling was performed either with replacement (wr) or without replacement (wor) depending on the needs of a particular protocol.
Functions to run the sampling protocols were developed in a compartmentalized manner to provide for substantial flexibility in exploring desired resampling protocols.
The order of operations (Figure \@ref(fig:length)) has the following schedule, with steps 1-3 being optional switches:

1.  Resample hauls (wr) from the set of hauls with associated catch per unit effort (in numbers)
2.  Within the resampled hauls from step 1, resample the observed lengths (wr)
3.  From the resampled lengths in step 2, subset the lengths (wor) with observed sex (either male or female) and sample these sex-length pairs at the sub-sampling level determined in step 2; equation \@ref(eq:eqn6)
4.  Calculate sex-specific population abundance at length, using equations \@ref(eq:eqn1) - \@ref(eq:eqn5)

The core of the bootstrap-simulation function (step 3 above) is designed to explore reductions in the sample size of lengths that are then sexed on a per haul basis.
In this bootstrap-simulation, the number of lengths in a given haul must be less than or equal to the desired sample size *x* determined in step 2.
In step 3, when the number of resampled lengths from step 2 in a haul $n_l$ is less than *x* then $n_l$ is used directly, if $n_l>x$ then a random draw of lengths is taken without replacement in step 3.

<!-- 1. reduce total number of length samples - wor  -->

<!-- 2. shuffle hauls - wr  -->

<!-- 3. shuffle lengths - wr  -->

<!-- 4. shuffle age data - wr -->

<!-- 5. resample lengths - wor  -->

<!-- 6. calculate length composition  -->

<!-- 7. calculate population abundance at length, using the results of step 6  -->

<!-- 8. calculate population abundance at age, using results of step 7 -->

<!-- 10. calculate effective samples size for ages and lengths, using results of step 7 and 8 -->

```{=tex}
\begin{equation}
 l_{x} = 
  \begin{cases}
    n_l    & n_l\le x  \\
    n_{lx} & n_l > x, ~ wor
  \end{cases}
  (\#eq:eqn6)
\end{equation}
```
The bootstrap-simulation then repeated steps 1-3 iteratively for each sex sub-sample size determined in step 1, providing iterated sex-specific population abundance at length that was then compared to the historical sex-specific population abundance at length determined by the bottom trawl surveys.

We applied the bootstrap-simulation to species that were most commonly captured in the EBS, AI, and GOA bottom trawl surveys (Tables \@ref(tab:ai-avg-samples) - \@ref(tab:goa-avg-samples)).
The sub-sample levels that we evaluated for subsequent sex determination from the length frequency collections were 50, 75, 100, 125, 150, and 175 samples.
We also ran the bootstrap-simulation for the historical number of sexed length frequency collections without subsetting in order to compare the base level uncertainty to the increase in uncertainty gained through sub-sampling.
We ran the bootstrap-simulation for 500 iterations, which was a level for which the variability in population abundance at length results had stabilized, and applied the bootstrap-simulation to the most recent 3 years of the respective bottom trawl surveys, which were the most indicative of the current sampling levels.
The bootstrap-simulation was developed in R [@Rcore] and is available via GitHub as an R package (<https://github.com/BenWilliams-NOAA/swo>).

## Computing effective and input sample size

[Including what we have in the tech memo to build on - Pete and Matt]

Effective sample size, as introduced by @Mcallister1997, is a statistic that can evaluate the level of intra-haul correlation in composition samples that are collected on a survey (whether from age or length frequency collections).
It is also a statistic that can evaluate the amount of uncertainty in an estimated composition compared to an observed composition.
Effective sample size is given by:

```{=tex}
\begin{equation}
 ESS=\frac{\sum_{c=1}^{C}E_c(1-E_c)}{\sum_{c=1}^{C}(E_c-O_c)^2}
 (\#eq:eqn7)
\end{equation}
```
where $E_c$ is the estimated proportion for category-*c* (which can be either age or length or any other arbitrary category across which proportions are computed) and $O_c$ is the observed proportion.

In this bootstrap-simulation the underlying length composition derived from the historical bottom trawl surveys was treated as the observed proportions $O_c$ in equation \@ref(eq:eqn7).
For each iteration of the bootstrap simulation for a determined sex sub-sample size we computed a sex-specific estimated proportion ($E_c$) that was then compared to the underlying historical sex-specific length composition (the effective sample size for the total length composition, as the sum of population abundance at length, was also computed).
Thus, across each iteration of the bootstrap simulation we computed an effective sample size that indicated the amount of increased uncertainty that was caused by sub-sampling sexed length frequency data.
To summarize effective sample size across iterations we used the harmonic mean, which has been shown to reduce bias in recovering the true sample size in simulations for a multinomial distribution.
Due to this reduction in bias the harmonic mean has also been recommended to determine the 'input sample size' that is used in stock assessment models to fit compositional data [@Stewart2014].
Herein, when we use the term 'effective sample size' we are referring to the effective sample sizes that were computed for each iteration of the bootstrap-simulation, when we use the term 'input sample size' we are referring to the harmonic mean of the iterated effective sample sizes.

# Results

[length results for selected species, including samples saved, ess and iss reductions - Pete and Ben/Meaghan]

[age results, including ess and iss reductions - Pete and Ben/Meaghan]

# Discussion

[Summary para of main results]

[Cost-benefit of precision compared to survey injuries]

[Still thinking on other paras]

# Acknowledgments

We thank *reviewer1* and *reviewer2* for their helpful reviews of this manuscript.

\newpage

# Citations

::: {#refs}
:::

\newpage

# Tables

[will determine which tables to include - kept this table in as place holder]

```{r ai-avg-samples}
knitr::kable(vroom::vroom(here::here('tables', 'ai_samples.csv')), caption = "Total length frequency samples from the most recent three Aleutian Islands surveys for the species evaluated in the bootstrap-simulation for reduction in sexed length-frequency collections.", align = c('lcccc'), format.args = list(big.mark = ",", scientific = FALSE))
```

\newpage

# Figures

[Will figure out what figures to include - pun intended, left length flowchart here as we will have the total flowchart included]

```{r length, fig.cap="Bootstrap-simulation flow chart, the steps refer to the order of operations as described in the *Bootstrap-simulation framework* section."}
knitr::include_graphics(here::here('figs', 'length_flowchart.png'))
```
